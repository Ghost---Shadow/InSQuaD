wandb:
  project: "quaild"
  name: "ideal_mpnet_stablelm"
  # entity: "carml"

architecture:
  generative_model:
    type: 'noop'
    checkpoint: 'none'
    device: 'none'
  
  semantic_search_model:
    type: 'mpnet'
    checkpoint: 'sentence-transformers/all-mpnet-base-v2'
    device: "cuda:0"

  subset_selection_strategy:
    type: 'ideal'
    k: 10 # (https://github.com/skzhang1/IDEAL/blob/23b1e1cb049004d8296c81933a692b49c42ab527/core_method.py#L470)
    rand_iter: 10 # https://github.com/skzhang1/IDEAL/blob/23b1e1cb049004d8296c81933a692b49c42ab527/core_method.py#L172

  dense_index:
    type: 'faiss'
    index_class: 'IndexFlatL2'
    repopulate_every: 'epoch'
    k_for_rerank: 10

  prompt_formatting_strategy:
    type: 'q_a_with_new_line'

# https://arxiv.org/pdf/2004.09297.pdf - Page 12
training:
  type: 'noop'
  dataset: 'dummy_hotpot_qa_with_q'
  q_d_tradeoff_lambda: 0.5
  extra_metrics: ['hotpot_qa_with_q_f1']
  epochs: 7
  batch_size: 32
  learning_rate: 3e-5
  weight_decay: 0.01
  learning_rate_decay_strategy: 'linear'
  warmup_ratio: 0.06
  seeds: [42]
  loss:
    type: 'graph_cut'
    lambd: 0.5

offline_validation:
  type: ideal
  generative_model:
    type: 'automodel'
    checkpoint: 'stabilityai/stablelm-2-1_6b'
    device: "cuda:0"
  q_d_tradeoff_lambda: 0.5
  datasets: ['mrpc', 'sst5','mnli', 'dbpedia', 'rte', 'hellaswag']
  seeds: [42]
  num_shots: 5
  # annotation_budget: 100 # 100 is waaaaay to slow, takes hours
  annotation_budget: 18 # 5-10 mins
  subsample_for_train_size: 3000
  subsample_for_eval_size: 256
