wandb:
  project: "quaild"
  name: "quaild_gc_mpnet_gpt2"
  # entity: "carml"

architecture:
  generative_model:
    type: 'noop'
    checkpoint: 'none'
    device: 'none'
  
  semantic_search_model:
    type: 'mpnet'
    checkpoint: 'sentence-transformers/all-mpnet-base-v2'
    device: "cuda:0"

  subset_selection_strategy:
    type: 'quaild_submodular'
    gain_cutoff: -0.0005 # Figure 4

  dense_index:
    type: 'faiss'
    index_class: 'IndexFlatL2'
    repopulate_every: 'epoch'
    k_for_rerank: 10

  prompt_formatting_strategy:
    type: 'q_a_with_new_line'

# https://arxiv.org/pdf/2004.09297.pdf - Page 12
training:
  type: 'noop'
  dataset: 'hotpot_qa_with_q'
  q_d_tradeoff_lambda: 0.5
  extra_metrics: ['hotpot_qa_with_q_f1']
  epochs: 7
  batch_size: 1 # 32 (Paper uses 32 with pretraining objective, our effective size is dynamic and larger)
  learning_rate: 1e-5
  weight_decay: 0.01
  learning_rate_decay_strategy: 'linear'
  warmup_ratio: 0.06
  seeds: [42]
  loss:
    type: 'graph_cut'
    lambd: 0.5

offline_validation:
  type: quaild_gain_counter
  generative_model:
    type: 'automodel'
    checkpoint: 'EleutherAI/gpt-neo-125m'
    device: "cuda:0"
  # datasets: ['mrpc', 'sst5','mnli', 'dbpedia', 'rte']
  datasets: ['mrpc']
  seeds: [42]
  num_shots: 5
  annotation_budget: 18
  subsample_for_train_size: 300
  subsample_for_eval_size: 256
